rewrite to fall back to cpu if gpu is unavailable

import torch

#check for gpu
if torch.backends.mps.is_available():
   mps_device = torch.device("mps")
   x = torch.ones(1, device=mps_device)
   print (x)
else:
   print ("MPS device not found.")

# GPU
import time
start_time = time.time()

# syncrocnize time with cpu, otherwise only time for oflaoding data to gpu would be measured
torch.mps.synchronize()

a = torch.ones(4000,4000, device="mps")
for _ in range(200):
   a +=a

elapsed_time = time.time() - start_time
print( "GPU Time: ", elapsed_time)
print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"CUDA Version: {torch.version.cuda}")
print(f"Number of GPUs: {torch.cuda.device_count()}")
import torch
if torch.cuda.is_available():
    device = torch.device("cuda")
    x = torch.rand(5, 5).to(device)
    print(f"Tensor on CUDA device: {x}")
else:
    print("CUDA is not available")
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from concurrent.futures import ThreadPoolExecutor, as_completed

# Check CUDA availability
if not torch.cuda.is_available():
    raise RuntimeError("CUDA is not available. Ensure PyTorch is installed with CUDA support.")

# Load model and tokenizer
model_name = 'distilbert-base-uncased-finetuned-sst-2-english'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Set up model for multi-GPU
if torch.cuda.device_count() > 1:
    model = torch.nn.DataParallel(model)

model.to('cuda')  # Move model to GPU

def classify_text(text):
    """Classify the given text and return results."""
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to('cuda')
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    predicted_class = torch.argmax(probabilities, dim=-1).item()
    return {
        'label': 'POSITIVE' if predicted_class == 1 else 'NEGATIVE',
        'score': probabilities[0][predicted_class].item()
    }

def split_text_into_chunks(text, max_length=512):
    """Split text into chunks of a specified maximum length."""
    tokens = tokenizer.encode(text, truncation=False)
    chunks = [tokenizer.decode(tokens[i:i + max_length], skip_special_tokens=True) 
              for i in range(0, len(tokens), max_length)]
    return chunks

def process_chunks_in_parallel(chunks):
    """Process text chunks in parallel using multiple GPUs."""
    results = []
    with ThreadPoolExecutor() as executor:
        future_to_chunk = {executor.submit(classify_text, chunk): chunk for chunk in chunks}
        for future in as_completed(future_to_chunk):
            chunk = future_to_chunk[future]
            try:
                result = future.result()
                results.append((chunk, result))
            except Exception as exc:
                print(f'Chunk processing generated an exception: {exc}')
    return results

# Test classification with simple text
test_texts = [
    "I love using GPUs for deep learning!",
    "I love using Hugging Face models!"
]

for text in test_texts:
    result = classify_text(text)
    print("\n")
    print(text)
    print(result)
    print("\n")

# Process text from file
transcribed_text_file = "cleaned_text.txt"

try:
    # Load transcribed text
    with open(transcribed_text_file, 'r') as file:
        transcribed_text = file.read()

    # Split text into chunks
    chunks = split_text_into_chunks(transcribed_text)

    # Process chunks in parallel
    results = process_chunks_in_parallel(chunks)

    # Print results
    for chunk, result in results:
        print("\n")
        print(chunk)
        print(result)
        print("\n")

except FileNotFoundError:
    print(f"The file '{transcribed_text_file}' was not found.")

except Exception as e:
    print(f"An error occurred: {e}")

ChatGPT said:
ChatGPT
Here's the revised script that falls back to CPU if GPU is unavailable, while maintaining the original functionality:

python
Copy code
import torch
import time
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from concurrent.futures import ThreadPoolExecutor, as_completed

# Check for MPS or CUDA availability
if torch.backends.mps.is_available():
    device = torch.device("mps")
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
    print("Neither MPS nor CUDA is available. Falling back to CPU.")

# Test GPU or CPU timing
start_time = time.time()

if device.type == "cpu":
    print("Using CPU.")
    # Run CPU benchmark
    a = torch.ones(4000, 4000, device=device)
    for _ in range(200):
        a += a
else:
    # Synchronize MPS or CUDA device with CPU
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()

    # Run GPU benchmark
    a = torch.ones(4000, 4000, device=device)
    for _ in range(200):
        a += a

elapsed_time = time.time() - start_time
print(f"{device.type.upper()} Time: {elapsed_time}")

print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"CUDA Version: {torch.version.cuda}")
print(f"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 0}")

# Check CUDA availability
if device.type == "cpu":
    raise RuntimeError("CUDA is not available. Ensure PyTorch is installed with CUDA support.")

# Load model and tokenizer
model_name = 'distilbert-base-uncased-finetuned-sst-2-english'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Set up model for multi-GPU if CUDA is available
if torch.cuda.device_count() > 1 and device.type == "cuda":
    model = torch.nn.DataParallel(model)

model.to(device)  # Move model to GPU or CPU

def classify_text(text):
    """Classify the given text and return results."""
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    predicted_class = torch.argmax(probabilities, dim=-1).item()
    return {
        'label': 'POSITIVE' if predicted_class == 1 else 'NEGATIVE',
        'score': probabilities[0][predicted_class].item()
    }

def split_text_into_chunks(text, max_length=512):
    """Split text into chunks of a specified maximum length."""
    tokens = tokenizer.encode(text, truncation=False)
    chunks = [tokenizer.decode(tokens[i:i + max_length], skip_special_tokens=True) 
              for i in range(0, len(tokens), max_length)]
    return chunks

def process_chunks_in_parallel(chunks):
    """Process text chunks in parallel using multiple GPUs if available."""
    results = []
    with ThreadPoolExecutor() as executor:
        future_to_chunk = {executor.submit(classify_text, chunk): chunk for chunk in chunks}
        for future in as_completed(future_to_chunk):
            chunk = future_to_chunk[future]
            try:
                result = future.result()
                results.append((chunk, result))
            except Exception as exc:
                print(f'Chunk processing generated an exception: {exc}')
    return results

# Test classification with simple text
test_texts = [
    "I love using GPUs for deep learning!",
    "I love using Hugging Face models!"
]

for text in test_texts:
    result = classify_text(text)
    print("\n")
    print(text)
    print(result)
    print("\n")

# Process text from file
transcribed_text_file = "cleaned_text.txt"

try:
    # Load transcribed text
    with open(transcribed_text_file, 'r') as file:
        transcribed_text = file.read()

    # Split text into chunks
    chunks = split_text_into_chunks(transcribed_text)

    # Process chunks in parallel
    results = process_chunks_in_parallel(chunks)

    # Print results
    for chunk, result in results:
        print("\n")
        print(chunk)
        print(result)
        print("\n")

except FileNotFoundError:
    print(f"The file '{transcribed_text_file}' was not found.")

except Exception as e:
    print(f"An error occurred: {e}")
Key Changes:
Device Detection: Added logic to fall back to CPU if neither MPS nor CUDA is available.
Benchmarking: Adjusted timing code to handle both GPU and CPU cases.
Device-Specific Operations: Modified synchronization and model placement to handle cases when MPS or CUDA is not available.
You said:
and pandas>=2.2.2
torch>=2.4.0
torchvision>=0.19.0
torchaudio>=2.4.0
transformers>=4.44.0
--extra-index-url https://download.pytorch.org/whl/nightly/cpu
ChatGPT said:
ChatGPT
To install the required packages with specific versions and the extra index URL for PyTorch, you can use the following pip command:

bash
Copy code
pip install pandas>=2.2.2 torch>=2.4.0 torchvision>=0.19.0 torchaudio>=2.4.0 transformers>=4.44.0 --extra-index-url https://download.pytorch.org/whl/nightly/cpu
Breakdown:
pandas>=2.2.2: Install pandas version 2.2.2 or newer.
torch>=2.4.0: Install PyTorch version 2.4.0 or newer.
torchvision>=0.19.0: Install torchvision version 0.19.0 or newer.
torchaudio>=2.4.0: Install torchaudio version 2.4.0 or newer.
transformers>=4.44.0: Install transformers version 4.44.0 or newer.
--extra-index-url https://download.pytorch.org/whl/nightly/cpu: Use this extra index URL to access the latest nightly builds of PyTorch and its related packages for CPU.
Make sure to run this command in your terminal or command prompt. If you are using a virtual environment, ensure it's activated before running the command.

You said:
now, rewrite both files to try MPS on MacOS first, then NVIDIA GPU next, then fall back to CPU if none of them are available
ChatGPT said:
ChatGPT
Memory updated
Here’s how you can rewrite the scripts to prioritize MPS on macOS, then NVIDIA GPU, and finally fall back to CPU if neither is available:

Script 1: Device Selection and Benchmarking
python
Copy code
import torch
import time

# Check for MPS, then CUDA, then fallback to CPU
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using MPS.")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using CUDA.")
else:
    device = torch.device("cpu")
    print("Neither MPS nor CUDA is available. Falling back to CPU.")

# Test device timing
start_time = time.time()

if device.type == "cpu":
    print("Running on CPU.")
    # Run CPU benchmark
    a = torch.ones(4000, 4000, device=device)
    for _ in range(200):
        a += a
else:
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()

    # Run GPU benchmark
    a = torch.ones(4000, 4000, device=device)
    for _ in range(200):
        a += a

elapsed_time = time.time() - start_time
print(f"{device.type.upper()} Time: {elapsed_time}")

print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"CUDA Version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}")
print(f"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 0}")
Script 2: Model Loading and Text Classification
python
Copy code
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from concurrent.futures import ThreadPoolExecutor, as_completed

# Check for MPS, then CUDA, then fallback to CPU
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using MPS.")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using CUDA.")
else:
    device = torch.device("cpu")
    print("Neither MPS nor CUDA is available. Falling back to CPU.")

# Load model and tokenizer
model_name = 'distilbert-base-uncased-finetuned-sst-2-english'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Set up model for multi-GPU if CUDA is available
if torch.cuda.device_count() > 1 and device.type == "cuda":
    model = torch.nn.DataParallel(model)

model.to(device)  # Move model to selected device

def classify_text(text):
    """Classify the given text and return results."""
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    predicted_class = torch.argmax(probabilities, dim=-1).item()
    return {
        'label': 'POSITIVE' if predicted_class == 1 else 'NEGATIVE',
        'score': probabilities[0][predicted_class].item()
    }

def split_text_into_chunks(text, max_length=512):
    """Split text into chunks of a specified maximum length."""
    tokens = tokenizer.encode(text, truncation=False)
    chunks = [tokenizer.decode(tokens[i:i + max_length], skip_special_tokens=True) 
              for i in range(0, len(tokens), max_length)]
    return chunks

def process_chunks_in_parallel(chunks):
    """Process text chunks in parallel using multiple GPUs if available."""
    results = []
    with ThreadPoolExecutor() as executor:
        future_to_chunk = {executor.submit(classify_text, chunk): chunk for chunk in chunks}
        for future in as_completed(future_to_chunk):
            chunk = future_to_chunk[future]
            try:
                result = future.result()
                results.append((chunk, result))
            except Exception as exc:
                print(f'Chunk processing generated an exception: {exc}')
    return results

# Test classification with simple text
test_texts = [
    "I love using GPUs for deep learning!",
    "I love using Hugging Face models!"
]

for text in test_texts:
    result = classify_text(text)
    print("\n")
    print(text)
    print(result)
    print("\n")

# Process text from file
transcribed_text_file = "cleaned_text.txt"

try:
    # Load transcribed text
    with open(transcribed_text_file, 'r') as file:
        transcribed_text = file.read()

    # Split text into chunks
    chunks = split_text_into_chunks(transcribed_text)

    # Process chunks in parallel
    results = process_chunks_in_parallel(chunks)

    # Print results
    for chunk, result in results:
        print("\n")
        print(chunk)
        print(result)
        print("\n")

except FileNotFoundError:
    print(f"The file '{transcribed_text_file}' was not found.")

except Exception as e:
    print(f"An error occurred: {e}")
Summary of Changes:
Device Selection: The code now tries MPS first, then CUDA, and falls back to CPU if neither is available.
Synchronization and Benchmarking: Adjusted to handle MPS, CUDA, and CPU accordingly.
Model and Tokenizer Loading: Ensured compatibility with the selected device and supports multi-GPU setup for CUDA.
This setup ensures your code will utilize the best available hardware, optimizing performance across different environments.

You said:
put it all in 1 python file classify_text.py and 1 requirements.txt file to run pip install -r requirements.txt
ChatGPT said:
ChatGPT
Sure! Here’s how you can organize everything into a single Python file and a requirements.txt file.

classify_text.py
python
Copy code
import torch
import time
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from concurrent.futures import ThreadPoolExecutor, as_completed

# Check for MPS, then CUDA, then fallback to CPU
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using MPS.")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using CUDA.")
else:
    device = torch.device("cpu")
    print("Neither MPS nor CUDA is available. Falling back to CPU.")

# Test device timing
start_time = time.time()

if device.type == "cpu":
    print("Running on CPU.")
    # Run CPU benchmark
    a = torch.ones(4000, 4000, device=device)
    for _ in range(200):
        a += a
else:
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()

    # Run GPU benchmark
    a = torch.ones(4000, 4000, device=device)
    for _ in range(200):
        a += a

elapsed_time = time.time() - start_time
print(f"{device.type.upper()} Time: {elapsed_time}")

print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"CUDA Version: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}")
print(f"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 0}")

# Load model and tokenizer
model_name = 'distilbert-base-uncased-finetuned-sst-2-english'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Set up model for multi-GPU if CUDA is available
if torch.cuda.device_count() > 1 and device.type == "cuda":
    model = torch.nn.DataParallel(model)

model.to(device)  # Move model to selected device

def classify_text(text):
    """Classify the given text and return results."""
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    predicted_class = torch.argmax(probabilities, dim=-1).item()
    return {
        'label': 'POSITIVE' if predicted_class == 1 else 'NEGATIVE',
        'score': probabilities[0][predicted_class].item()
    }

def split_text_into_chunks(text, max_length=512):
    """Split text into chunks of a specified maximum length."""
    tokens = tokenizer.encode(text, truncation=False)
    chunks = [tokenizer.decode(tokens[i:i + max_length], skip_special_tokens=True) 
              for i in range(0, len(tokens), max_length)]
    return chunks

def process_chunks_in_parallel(chunks):
    """Process text chunks in parallel using multiple GPUs if available."""
    results = []
    with ThreadPoolExecutor() as executor:
        future_to_chunk = {executor.submit(classify_text, chunk): chunk for chunk in chunks}
        for future in as_completed(future_to_chunk):
            chunk = future_to_chunk[future]
            try:
                result = future.result()
                results.append((chunk, result))
            except Exception as exc:
                print(f'Chunk processing generated an exception: {exc}')
    return results

# Test classification with simple text
test_texts = [
    "I love using GPUs for deep learning!",
    "I love using Hugging Face models!"
]

for text in test_texts:
    result = classify_text(text)
    print("\n")
    print(text)
    print(result)
    print("\n")

# Process text from file
transcribed_text_file = "cleaned_text.txt"

try:
    # Load transcribed text
    with open(transcribed_text_file, 'r') as file:
        transcribed_text = file.read()

    # Split text into chunks
    chunks = split_text_into_chunks(transcribed_text)

    # Process chunks in parallel
    results = process_chunks_in_parallel(chunks)

    # Print results
    for chunk, result in results:
        print("\n")
        print(chunk)
        print(result)
        print("\n")

except FileNotFoundError:
    print(f"The file '{transcribed_text_file}' was not found.")

except Exception as e:
    print(f"An error occurred: {e}")
requirements.txt
shell
Copy code
pandas>=2.2.2
torch>=2.4.0
torchvision>=0.19.0
torchaudio>=2.4.0
transformers>=4.44.0
--extra-index-url https://download.pytorch.org/whl/nightly/cpu
Instructions
Create the Files: Save the provided code into classify_text.py and requirements.txt files.

Install Dependencies:

Open a terminal and navigate to the directory containing the requirements.txt file.
Run:
bash
Copy code
pip install -r requirements.txt
Run the Script:

In the terminal, execute the script using:
bash
Copy code
python classify_text.py
This setup ensures your environment has all the necessary dependencies and runs the provided script with proper device selection and handling.
